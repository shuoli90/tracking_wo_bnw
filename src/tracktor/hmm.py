# -*- coding: utf-8 -*-
"""HMM_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6lqV4-r5x0w5wMMkztCGLeOKEdfybbK

# HMM
"""
# import library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from itertools import product
from functools import reduce
import pickle
from numpy import array
from numpy import argmax
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder

def lst2str(o):
  tmp = []
  for lst in o:
    tmp.append(' '.join(map(str, lst)))
  return tmp

def str2lst(a, objects):
  tmp = []
  for s in a:    
    lst = list(map(float, s.split(' ')))
    tmp.append(lst)
  ret = [label_encoder.inverse_transform([argmax(array(tmp)[i, :])])[0] for i in range(len(a))]
  return ret
def one_hot_encode(objects):
    enc = OneHotEncoder(handle_unknown='ignore')
    values = array(objects)
    label_encoder = LabelEncoder()
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(values)
    # print("after integer label encoding: {}".format(integer_encoded))
    # binary encode
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
    # print("after one-hot encoding: \n{}".format(onehot_encoded))
    # invert first example
    # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])
    # print(inverted)
    states =  lst2str(onehot_encoded)
    return states

class ProbabilityVector:
    def __init__(self, probabilities: dict):
        states = probabilities.keys()
        probs  = probabilities.values()
        
        assert len(states) == len(probs)
            # "The probabilities must match the states."
        assert len(states) == len(set(states))
            # "The states must be unique."
       
        assert abs(sum(probs) - 1.0) < 1e-12
            #"Probabilities must sum up to 1."
        # print(len(list(filter(lambda x: 0 <= x <= 1, probs))))
        # print("===========")
        # print(probs)
        assert len(list(filter(lambda x: 0 <= x <= 1.1, probs))) == len(probs)
            # "Probabilities must be numbers from [0, 1] interval."
        
        self.states = sorted(probabilities)
        # self.states = ['0.0 1.0', '1.0 0.0']
        
        self.values = np.array(list(map(lambda x: probabilities[x], self.states))).reshape(1, -1)
        # print(self.values)
        # print("=============")
        
    @classmethod
    def initialize(cls, states: list):
        size = len(states)
        rand = np.random.rand(size) / (size**2) + 1 / size
        rand /= rand.sum(axis=0)
        return cls(dict(zip(states, rand)))
    
    @classmethod
    def from_numpy(cls, array: np.ndarray, state: list):
        return cls(dict(zip(states, list(array))))

    @property
    def dict(self):
        return {k:v for k, v in zip(self.states, list(self.values.flatten()))}

    @property
    def df(self):
        return pd.DataFrame(self.values, columns=self.states, index=['probability'])

    def __repr__(self):
        return "P({}) = {}.".format(self.states, self.values)

    def __eq__(self, other):
        if not isinstance(other, ProbabilityVector):
            raise NotImplementedError
        if (self.states == other.states) and (self.values == other.values).all():
            return True
        return False

    def __getitem__(self, state: str) -> float:
        # print(state)
        # print(self.states)
        if state not in self.states:
            raise ValueError("Requesting unknown probability state from vector.")
        index = self.states.index(state)
        return float(self.values[0, index])

    def __mul__(self, other) -> np.ndarray:
        if isinstance(other, ProbabilityVector):
            return self.values * other.values
        elif isinstance(other, (int, float)):
            return self.values * other
        else:
            NotImplementedError

    def __rmul__(self, other) -> np.ndarray:
        return self.__mul__(other)

    def __matmul__(self, other) -> np.ndarray:
        if isinstance(other, ProbabilityMatrix):
            return self.values @ other.values

    def __truediv__(self, number) -> np.ndarray:
        if not isinstance(number, (int, float)):
            raise NotImplementedError
        x = self.values
        return x / number if number != 0 else x / (number + 1e-12)

    def argmax(self):
        index = self.values.argmax()
        return self.states[index]

class ProbabilityMatrix:
    def __init__(self, prob_vec_dict: dict):
        
        assert len(prob_vec_dict) > 1, \
            "The numebr of input probability vector must be greater than one."
        assert len(set([str(x.states) for x in prob_vec_dict.values()])) == 1, \
            "All internal states of all the vectors must be indentical."
        assert len(prob_vec_dict.keys()) == len(set(prob_vec_dict.keys())), \
            "All observables must be unique."

        self.states      = sorted(prob_vec_dict)
        self.observables = prob_vec_dict[self.states[0]].states
        self.values      = np.stack([prob_vec_dict[x].values \
                           for x in self.states]).squeeze() 

    @classmethod
    def initialize(cls, states: list, observables: list):
        size = len(states)
        rand = np.random.rand(size, len(observables)) \
             / (size**2) + 1 / size
        rand /= rand.sum(axis=1).reshape(-1, 1)
        aggr = [dict(zip(observables, rand[i, :])) for i in range(len(states))]
        pvec = [ProbabilityVector(x) for x in aggr]
        return cls(dict(zip(states, pvec)))

    @classmethod
    def from_numpy(cls, array: 
                  np.ndarray, 
                  states: list, 
                  observables: list):
        p_vecs = [ProbabilityVector(dict(zip(observables, x))) \
                  for x in array]
        return cls(dict(zip(states, p_vecs)))

    @property
    def dict(self):
        return self.df.to_dict()

    @property
    def df(self):
        return pd.DataFrame(self.values, 
               columns=self.observables, index=self.states)

    def __repr__(self):
        return "PM {} states: {} -> obs: {}.".format(
            self.values.shape, self.states, self.observables)

    def __getitem__(self, observable: str) -> np.ndarray:
        # print(observable)
        # print(self.observables)
        if observable not in self.observables:
            raise ValueError("Requesting unknown probability observable from the matrix.")
        index = self.observables.index(observable)
        return self.values[:, index].reshape(-1, 1)

class HiddenMarkovChain:
    def __init__(self, T, E, pi):
        self.T = T  # transmission matrix A
        self.E = E  # emission matrix B
        self.pi = pi
        self.states = pi.states
        self.observables = E.observables
    
    def __repr__(self):
        return "HML states: {} -> observables: {}.".format(
            len(self.states), len(self.observables))
    
    @classmethod
    def initialize(cls, states: list, observables: list):
        a = np.array([[1, 0],[0, 1]])
        b = np.array([[0.9, 0.1],[0.1, 0.9]])
        T = ProbabilityMatrix.from_numpy(a, states, states)
        E = ProbabilityMatrix.from_numpy(b, states, observables)
        # T = ProbabilityMatrix.initialize(states, states)
        # E = ProbabilityMatrix.initialize(states, observables)
        pi = ProbabilityVector.initialize(states)
        return cls(T, E, pi)
    
    def _create_all_chains(self, chain_length):
        return list(product(*(self.states,) * chain_length))
    
    def score(self, observations: list) -> float:
        def mul(x, y): return x * y
        
        score = 0
        all_chains = self._create_all_chains(len(observations))
        for idx, chain in enumerate(all_chains):
            expanded_chain = list(zip(chain, [self.T.states[0]] + list(chain)))
            expanded_obser = list(zip(observations, chain))
            
            p_observations = list(map(lambda x: self.E.df.loc[x[1], x[0]], expanded_obser))
            p_hidden_state = list(map(lambda x: self.T.df.loc[x[1], x[0]], expanded_chain))
            p_hidden_state[0] = self.pi[chain[0]]
            
            score += reduce(mul, p_observations) * reduce(mul, p_hidden_state)
        return score

class HiddenMarkovChain_FP(HiddenMarkovChain):
    def _alphas(self, observations: list) -> np.ndarray:
        alphas = np.zeros((len(observations), len(self.states)))
        alphas[0, :] = self.pi.values * self.E[observations[0]].T
        for t in range(1, len(observations)):
            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1) 
                         @ self.T.values) * self.E[observations[t]].T
        return alphas
    
    def score(self, observations: list) -> float:
        alphas = self._alphas(observations)
        return float(alphas[-1].sum())

class HiddenMarkovChain_Simulation(HiddenMarkovChain):
    def run(self, length: int) -> (list, list):
        assert length >= 0, "The chain needs to be a non-negative number."
        s_history = [0] * (length + 1)
        o_history = [0] * (length + 1)
        
        prb = self.pi.values
        obs = prb @ self.E.values
        s_history[0] = np.random.choice(self.states, p=prb.flatten())
        o_history[0] = np.random.choice(self.observables, p=obs.flatten())
        
        for t in range(1, length + 1):
            prb = prb @ self.T.values
            obs = prb @ self.E.values
            s_history[t] = np.random.choice(self.states, p=prb.flatten())
            o_history[t] = np.random.choice(self.observables, p=obs.flatten())
        
        return o_history, s_history

class HiddenMarkovChain_Uncover(HiddenMarkovChain_Simulation):
    def _alphas(self, observations: list) -> np.ndarray:
        alphas = np.zeros((len(observations), len(self.states)))
        alphas[0, :] = self.pi.values * self.E[observations[0]].T
        for t in range(1, len(observations)):
            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1) @ self.T.values) \
                         * self.E[observations[t]].T
        return alphas
    
    def _betas(self, observations: list) -> np.ndarray:
        betas = np.zeros((len(observations), len(self.states)))
        betas[-1, :] = 1
        for t in range(len(observations) - 2, -1, -1):
            betas[t, :] = (self.T.values @ (self.E[observations[t + 1]] \
                        * betas[t + 1, :].reshape(-1, 1))).reshape(1, -1)
        return betas
    
    def uncover(self, observations: list) -> list:
        alphas = self._alphas(observations)
        betas = self._betas(observations)
        maxargs = (alphas * betas).argmax(axis=1)
        return list(map(lambda x: self.states[x], maxargs))

class HiddenMarkovLayer(HiddenMarkovChain_Uncover):
    def _digammas(self, observations: list) -> np.ndarray:
        L, N = len(observations), len(self.states)
        digammas = np.zeros((L - 1, N, N))

        alphas = self._alphas(observations)
        betas = self._betas(observations)
        score = self.score(observations)
        for t in range(L - 1):
            P1 = (alphas[t, :].reshape(-1, 1) * self.T.values)
            P2 = self.E[observations[t + 1]].T * betas[t + 1].reshape(1, -1)
            digammas[t, :, :] = P1 * P2 / score
        return digammas

class HiddenMarkovModel:
    def __init__(self, hml: HiddenMarkovLayer):
        self.layer = hml
        self._score_init = 0
        self.score_history = []

    @classmethod
    def initialize(cls, states: list, observables: list):
        layer = HiddenMarkovLayer.initialize(states, observables)
        return cls(layer)

    def update(self, observations: list) -> float:
        alpha = self.layer._alphas(observations)
        beta = self.layer._betas(observations)
        digamma = self.layer._digammas(observations)
        score = alpha[-1].sum()
        gamma = alpha * beta / score 

        L = len(alpha)
        obs_idx = [self.layer.observables.index(x) \
                  for x in observations]
        capture = np.zeros((L, len(self.layer.states), len(self.layer.observables)))
        for t in range(L):
            capture[t, :, obs_idx[t]] = 1.0

        pi = gamma[0]
        T = digamma.sum(axis=0) / gamma[:-1].sum(axis=0).reshape(-1, 1)
        E = (capture * gamma[:, :, np.newaxis]).sum(axis=0) / gamma.sum(axis=0).reshape(-1, 1)

        self.layer.pi = ProbabilityVector.from_numpy(pi, self.layer.states)
        self.layer.T = ProbabilityMatrix.from_numpy(T, self.layer.states, self.layer.states)
        self.layer.E = ProbabilityMatrix.from_numpy(E, self.layer.states, self.layer.observables)
            
        return score

    def train(self, observations: list, epochs: int, tol=None):
        self._score_init = 0
        self.score_history = (epochs + 1) * [0]
        early_stopping = isinstance(tol, (int, float))

        for epoch in range(1, epochs + 1):
            score = self.update(observations)
            print("Training... epoch = {} out of {}, score = {}.".format(epoch, epochs, score))
            if early_stopping and abs(self._score_init - score) / score < tol:
                print("Early stopping.")
                break
            self._score_init = score
            self.score_history[epoch] = score

    




"""# Sample Usage"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/tracktor_data

# with open('boxes', 'rb') as f:
#     boxes = pickle.load(f)

# with open('scores', 'rb') as f:
#     scores = pickle.load(f)

# with open('logvars', 'rb') as f:
#     logvars = pickle.load(f)

# boxes

"""# Visibility HMM"""

# bb_index = 6
# T = 10
# observations= []

# for i in range(100, 100+T):
#   tmp = scores[i][bb_index]
#   if tmp > 0.5: 
#     observations.append('visible')
#   else:
#     observations.append('invisible')
# print(observations)

# one-hot encoding

# enc = OneHotEncoder(handle_unknown='ignore')
# visibility = ['visible', 'invisible']
# values = array(visibility)
# print("ground truth visibility: {}".format(values))
# # integer encode
# label_encoder = LabelEncoder()
# integer_encoded = label_encoder.fit_transform(values)
# print("after integer label encoding: {}".format(integer_encoded))
# # binary encode
# onehot_encoder = OneHotEncoder(sparse=False)
# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
# print("after one-hot encoding: \n{}".format(onehot_encoded))
# # invert first example
# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])
# # print(inverted)
# states =  lst2str(onehot_encoded)
# # print(states)


# observables = visibility
# print(observables)

# np.random.seed(42)

# hml = HiddenMarkovLayer.initialize(states, observables)
# print(hml.T.df)
# print(hml.E.df)
# hmm = HiddenMarkovModel(hml)
# tmp = hmm.layer.uncover(observations)
# print("latent objects predictions are: {}".format(str2lst(tmp, objects)))

"""# Label HMM

## Inference function

label given distribution as observation: scores from faster RCNN
"""

# scores[0].shape




def label_tracking_hmm(scores):
    # bb_index = 6
    # T = 10
    observations= []
    # breakpoint()
    # for i in range(scores.shape[0]):
    # tmp = scores[i]
    if scores > 0.5: 
        # return 'pedestrian'
        observations.append('pedestrian')
    else:
        # return 'background'
        observations.append('background')
        # observations = lst2str(obs)
        # print(observations)


    # # one-hot encoding

    enc = OneHotEncoder(handle_unknown='ignore')
    objects = ['pedestrian', 'background']
    values = array(objects)
    # print("ground truth objects: {}".format(values))
    # integer encode
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(values)
    # print("after integer label encoding: {}".format(integer_encoded))
    # binary encode
    onehot_encoder = OneHotEncoder(sparse=False)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
    # print("after one-hot encoding: \n{}".format(onehot_encoded))
    # invert first example
    inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])
    # print(inverted)
    states =  lst2str(onehot_encoded)
    # print(states)


    observables = objects
    # print(observables)

    np.random.seed(42)

    hml = HiddenMarkovLayer.initialize(states, observables)
    # print(hml.T.df)
    # print(hml.E.df)
    hmm = HiddenMarkovModel(hml)
    # tmp = hmm.layer.uncover(observations)
    return hmm, observations
    # print("latent objects predictions are: {}".format(str2lst(tmp, objects)))

# """HMM can make consistent prediction."""

# a = [scores[i][bb_index] for i in range(100,100+T)]
# df = pd.DataFrame(np.array([a, [0.5]*T]).T,columns=['softmax score', 'threshold'])
# df.plot()

# """## Train function"""

# np.random.seed(42)

# hml = HiddenMarkovLayer.initialize(states, observables)
# hmm = HiddenMarkovModel(hml)

# hmm.train(observations, 10)

# RUNS = 100000
# T = 5

# chains = RUNS * [0]
# for i in range(len(chains)):
#     chain = hmm.layer.run(T)[0]
#     chains[i] = '-'.join(chain)

# df = pd.DataFrame(pd.Series(chains).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})
# df = pd.merge(df, df['chain'].str.split('-', expand=True), left_index=True, right_index=True)

# s = []
# for i in range(T + 1):
#     s.append(df.apply(lambda x: x[i] == observations[i], axis=1))
# # counts is the frequency of occurence
# df['matched'] = pd.concat(s, axis=1).sum(axis=1)
# df['counts'] = df['counts'] / RUNS * 100
# df = df.drop(columns=['chain'])
# df.head(30)

# hml_rand = HiddenMarkovLayer.initialize(states, observables)
# hmm_rand = HiddenMarkovModel(hml_rand)

# RUNS = 10000
# T = 5

# chains_rand = RUNS * [0]
# for i in range(len(chains_rand)):
#     chain_rand = hmm_rand.layer.run(T)[0]
#     chains_rand[i] = '-'.join(chain_rand)

# df2 = pd.DataFrame(pd.Series(chains_rand).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})
# df2 = pd.merge(df2, df2['chain'].str.split('-', expand=True), left_index=True, right_index=True)

# s = []
# for i in range(T + 1):
#     s.append(df2.apply(lambda x: x[i] == observations[i], axis=1))

# df2['matched'] = pd.concat(s, axis=1).sum(axis=1)
# df2['counts'] = df2['counts'] / RUNS * 100
# df2 = df2.drop(columns=['chain'])

# fig, ax = plt.subplots(1, 1, figsize=(14, 6))

# ax.plot(df['matched'], 'g:')
# ax.plot(df2['matched'], 'k:')

# ax.set_xlabel('Ordered index')
# ax.set_ylabel('Matching observations')
# ax.set_title('Verification on a 6-observation chain.')

# ax2 = ax.twinx()
# ax2.plot(df['counts'], 'r', lw=3)
# ax2.plot(df2['counts'], 'k', lw=3)
# ax2.set_ylabel('Frequency of occurrence [%]')

# ax.legend(['trained', 'initialized'])
# ax2.legend(['trained', 'initialized'])

# plt.grid()
# plt.show()

# def str2lst(a, objects):
#   tmp = []
#   for s in a:    
#     lst = list(map(float, s.split(' ')))
#     tmp.append(lst)
#   ret = [label_encoder.inverse_transform([argmax(array(tmp)[i, :])])[0] for i in range(len(a))]
#   return ret

# a = hmm.layer.uncover(observations)
# print("latect objects predictions are: {}".format(str2lst(a, objects)))

# T = 10
# cnt = 0
# for i in range(T):
#   gt = 'pedestrian' if scores[i][bb_index] > 0.5 else 'non-pedestrain'
#   if gt == str2lst(a, objects)[i]:
#     cnt += 1
# print("accuracy is: {}".format(cnt/T))

